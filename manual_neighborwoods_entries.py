# -*- coding: utf-8 -*-
"""Manual_NeighborWoods_Entries.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qNoRqLJwB_7bC7nTzYla4n_YIgPpwUi4

### Values to Change
"""

# Enter the values from the Website sign ups
Street_Num = '108'
Street_Name = 'Woodrow Ave'
source_type = 'Website'
First_Name = 'Jannira'
Last_Name = 'Rodriguez'
Input_Date = '7/26/24 18:55:00'
User_Neighborhood = 'Dorchester'
User_Zip = '02124'
Email = 'jaly1223@gmail.com'
Phone = '(857) 452-4987'
Contact = 'Text, Email'
Landowner = 'Yes landowner'

"""### Prep Workspace"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

# Import required modules
import pandas as pd
import numpy as np
import gspread
import string
from oauth2client.service_account import ServiceAccountCredentials

# Define the scope
scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]

# Provide the path to the JSON file
creds = ServiceAccountCredentials.from_json_keyfile_name('/content/drive/Shareddrives/SFTT Shared Drive/NeighborWoods Data Management/enhanced-keel-424914-m2-7c2e82d072b9.json', scope)

# Authorize the clientsheet
client = gspread.authorize(creds)

"""### Read In Data"""

# Read in the merged geodatabases

file_path = '/content/drive/Shareddrives/SFTT Shared Drive/NeighborWoods Data Management/Merged_Geodatabse.csv'

dtype_spec = {
    'addr_str': str,
    'addr_num': str,
    'MAP_PAR_ID': str,
    'muni': str,
    'site_addr': str,
    'addr_zip': str,
    'tes': str,
    'LU_Recode_': str,
}

merged_df = pd.read_csv(file_path, dtype=dtype_spec, low_memory=False)

# New Tree Planting Progress Database
TPP_url = 'https://docs.google.com/spreadsheets/d/15TbiooJKm0yN6qaZsuhguYETzz_M3g4UM5RdwW_Zn3E/edit?gid=0#gid=0'
TPP_Sheets = client.open_by_url(TPP_url)

TPP_Sheet4 = TPP_Sheets.worksheet("Website Sign Ups")

TPP_Website = TPP_Sheet4.get_all_values()

TPP_Website_df = pd.DataFrame(TPP_Website[1:], columns = TPP_Website[0])

"""###Linking Data

Prepares Dataframe for linking
"""

# Capitalize street name
Street_Name = Street_Name.upper()

# Create a dataframe for entry
Manual_Entry_df = pd.DataFrame({
    'Street Number': [Street_Num],
    'Street Name': [Street_Name],
    'Source Type': [source_type],
    'First Name': [First_Name],
    'Last Name': [Last_Name],
    'Input Date': [Input_Date],
    'User Neighborhood': [User_Neighborhood],
    'User Zip': [User_Zip],
    'Email': [Email],
    'Phone': [Phone],
    'Contact': [Contact],
    'Landowner': [Landowner]
})

print(Manual_Entry_df.columns.tolist())

# Define number of letters to shrink by
shrink = len(Street_Name) - 2

# Add a shortened address column to each dataframe
# Eliminates confusion with suffix
Manual_Entry_df['short_street_name'] = Street_Name[:shrink]
merged_df.loc[:, 'short_str_name_AG'] = merged_df['addr_str'].str[:shrink]

# Capitalize address column
Manual_Entry_df['Street Name'] = Manual_Entry_df['Street Name'].str.upper()

# Initialize a column for ID_Num
Manual_Entry_df['ID_Num'] = np.nan

"""Creates a function that provides linking column between our data and the ArcGIS data"""

# Create a function that links the tables
def link_address(Manual_Entry_row, merged_df, idx, shrink):
  # Select street number and name
  street_number = Manual_Entry_row['Street Number']
  str_name_manual = Manual_Entry_row['Street Name']

  # Shorten street name
  short_str_name_manual = str_name_manual[:shrink]
  merged_df.loc[:, 'short_str_name_AG'] = merged_df['addr_str'].str[:shrink]

  # Find the rows that contain that street name
  matching_rows = merged_df[merged_df['short_str_name_AG'].str.contains(short_str_name_manual, na=False)]

  # Find the right number
  matching_numbers = matching_rows['addr_num'].str.contains(street_number, na=False)
  matching_numbers_rows = matching_rows[matching_numbers]

  # Narrow down number rows
  # Add ID_Num to NW
  right_index = None

  if len(matching_numbers_rows) > 1:
    if matching_numbers_rows['addr_num'].str.contains(' ', na=False).any():
      individual_numbers = matching_numbers_rows[['addr_num', 'ID_Num']].apply(lambda row: row['addr_num'].split() if row['addr_num'] is not None and ' ' in row['addr_num'] else [row['addr_num']], axis=1)

      for idx2, num_list in individual_numbers.items():
        if street_number in num_list:
          right_index = idx2
          right_ID_Num = matching_numbers_rows.loc[right_index, 'ID_Num']
          return right_ID_Num

    else:
      for idx3, row in matching_numbers_rows.iterrows():
        if street_number == row['addr_num']:
          right_index = idx3
          right_ID_Num = matching_numbers_rows.loc[right_index, 'ID_Num']
          return right_ID_Num

  else:
      if not matching_numbers_rows.empty:
          right_index = matching_numbers_rows.index[0]
          right_ID_Num = matching_numbers_rows.loc[right_index, 'ID_Num']
          return right_ID_Num

  # Add NaN value if no address was found
  if right_index is None:
      return np.NaN

# Create a vector that starts at 10 and goes to 5
shrink_range = range(shrink, 5, -1)

# Turn Manual Entry into a row
Manual_Entry_row = Manual_Entry_df.loc[0]

# Iterate through the database multiple times to make sure large names and small names find matches
# Stops when
for shrink_i in shrink_range:
    id_num = link_address(Manual_Entry_row, merged_df, 0, shrink_i)
    Manual_Entry_df.at[0, 'ID_Num'] = id_num

"""Full merge of all columns"""

# Ensure the ID_Num column in both DataFrames is of the same type
Manual_Entry_df['ID_Num'] = Manual_Entry_df['ID_Num'].astype('Int64')
merged_df['ID_Num'] = merged_df['ID_Num'].astype('Int64')

# Merge the dataframes on the ID_Num column
Full_df = Manual_Entry_df.merge(merged_df, on='ID_Num', how='left', suffixes=('_NW', '_AG'))

"""###Create columns"""

# Add in website for all rows under the Source column
Full_df['Source'] = source_type

# Change City column to first letter capitals
Full_df['CITY'] = Full_df['CITY'].str.title()

# Create a list of acceptable neighborhoods
neighborhood_list = ['Dorchester','Hyde Park', 'Roxbury', 'Mattapan']

# Change Landowner to True False
if 'Yes' in Full_df.at[0, 'Landowner']:
  Full_df.at[0, 'Landowner'] = 'TRUE'
else:
  Full_df.at[0, 'Landowner'] = 'FALSE'

# Add Available Area (sqm) column
Full_df['Lot Area (sqm)'] = (Full_df['sqm_imperv'] * 100)/ Full_df['pct_imperv']
Full_df['Available Area (sqm)'] = Full_df['Lot Area (sqm)'] - Full_df['sqm_imperv']

# Combine the street number and name columns

Full_df['User Address'] = Full_df['Street Number'] + ' ' + Full_df['Street Name']

# Handle Duplicate Rows

# Filter out rows with duplicate dates
Full_df = Full_df.drop_duplicates(subset='Input Date', keep='first')

# Reset index
Full_df = Full_df.reset_index(drop=True)

# Print Full_df columns
print(Full_df.columns.tolist())

"""###Rename columns"""

# Choose columns to save
selected_columns = [
    'Source',
    'Input Date',
    'First Name',
    'Last Name',
    'User Address',
    'User Neighborhood',
    'User Zip',
    'Email',
    'Phone',
    'Contact',
    'Landowner',
    'MAP_PAR_ID',
    'muni',
    'site_addr',
    'addr_str',
    'addr_num',
    'addr_zip',
    'CanopyPerc',
    'CITY',
    'Lot Area (sqm)',
    'pct_imperv',
    'tes',
    'PrioZone',
    'HisDis (Historical Disinvestment)',
    'EJ Flag',
    'EJ #',
    'Available Area (sqm)',
    'HisDis Letter',
    'LU_Recode_',
    'ID_Num',
    'sqm_imperv']

new_Full_df = Full_df[selected_columns].copy()

# Rename columns
new_column_names = {
    'Source': 'Source',
    'Input Date': 'Input Date',
    'First Name': 'First Name',
    'Last Name': 'Last Name',
    'User Address': 'Address (Website)',
    'User Neighborhood': 'User Neighborhood',
    'User Zip': 'Zip Code',
    'Email': 'Email',
    'Phone': 'Phone Number',
    'Contact': 'Best way to contact',
    'Number of trees': 'Number of trees',
    'Landowner': 'Landowner?',
    'ID_Num': 'ID_Num',
    'MAP_PAR_ID': 'Map Parcel ID',
    'muni': 'Municipality',
    'site_addr': 'Site Address (ArcGIS)',
    'addr_str': 'Street Name (ArcGIS)',
    'addr_num': 'Street Number (ArcGIS)',
    'addr_zip': 'Street Zip (ArcGIS)',
    'sqm_imperv': 'Impervious Area (sqm)',
    'CanopyPerc': 'Canopy Percentage (ArcGIS)',
    'CITY': 'Neighborhood (ArcGIS)',
    'Lot Area (sqm)': 'Lot Area (sqm)',
    'pct_imperv': 'Impervious Percentage (ArcGIS)',
    'tes': 'TES (Census block)',
    'PrioZone': 'UFPZ (Urban Forest Priority Zone)',
    'HisDis (Historical Disinvestment)': 'HisDis (Historical Disinvestment)',
    'EJ Flag': 'EJ Flag',
    'EJ #': 'EJ #',
    'Available Area (sqm)': 'Available Area (sqm)',
    'HisDis Letter': 'HisDis Letter',
    'LU_Recode_': 'Type of Property'
}

new_Full_df.rename(columns=new_column_names, inplace=True)

"""### Writing Data to Sheet"""

# Create a list of letters that rolls over to AA

multiple_letters_list = []
for i in range(1, 3):  # Go through once for singletons and a second time for doubles
        for letter in string.ascii_uppercase: # Repeat for each uppercase letter
            if i == 1:
                multiple_letters_list.append(letter)
            else:
                for letter2 in string.ascii_uppercase: # Repeat for each uppercase letter
                    multiple_letters_list.append(letter + letter2)

"""Back End"""

# List of columns to select for Back_End_df
selected_columns = [
    'ID_Num',
    'Source',
    'Input Date',
    'First Name',
    'Last Name',
    'Phone Number',
    'Email',
    'Best way to contact',
    'Address (Website)',
    'Site Address (ArcGIS)',
    'Street Number (ArcGIS)',
    'Street Name (ArcGIS)',
    'User Neighborhood',
    'Neighborhood (ArcGIS)',
    'Municipality',
    'Zip Code',
    'Street Zip (ArcGIS)',
    'Map Parcel ID',
    'UFPZ (Urban Forest Priority Zone)',
    'HisDis (Historical Disinvestment)',
    'HisDis Letter',
    'EJ Flag',
    'EJ #',
    'Available Area (sqm)',
    'Lot Area (sqm)',
    'Impervious Area (sqm)',
    'Impervious Percentage (ArcGIS)',
    'Landowner?',
    'TES (Census block)',
    'Canopy Percentage (ArcGIS)',
    'Type of Property'
]

# Create Back End Dataframe
Back_End_df = new_Full_df[selected_columns].copy()

# Prep for writing to Google Sheet
sheet = client.open('Yardtree Planting Progress Database Neighborwoods')
Back_End = sheet.worksheet('Back End - NeighborWoods')

# Get data from Back End Sheet
existing_headers = Back_End.row_values(1)
existing_data = Back_End.get_all_values()

# Slice data to select the data from just the B:AE rows
start_col_index = 1  # Column B
end_col_index = 31  # Column AE

# Slice headers to include columns B through AE
existing_headers_slice = existing_headers[start_col_index:end_col_index + 1]

# Slice data to include columns B through AE
existing_data_slice = [row[start_col_index:end_col_index + 1] for row in existing_data]

existing_df = pd.DataFrame(existing_data_slice[1:], columns = existing_headers_slice)

# Get columns right
for col in existing_headers_slice:
    if col not in Back_End_df.columns:
        Back_End_df[col] = None

Back_End_df = Back_End_df[existing_headers_slice]

# Replace NaN and infinity values with None
Back_End_df = Back_End_df.replace({np.nan: None, np.inf: None, -np.inf: None})

# Convert DataFrame to a list of lists, excluding the header
data_list = Back_End_df.values.tolist()

# Find the first empty row in the "Input Date" column
input_date_col_idx = existing_headers_slice.index("Input Date")
start_row = next((idx for idx, row in enumerate(existing_data[1:], start=2) if not row[input_date_col_idx]), len(existing_data) + 1)

# Prepare the range for updating (starting at column B)
start_col = 'B'
end_col = multiple_letters_list[len(data_list[0])]
range_to_update = f'{start_col}{start_row}:{end_col}{start_row + len(data_list) - 1}'

# Append the data to the Google Sheet without the header
Back_End.update(range_to_update, data_list, value_input_option='USER_ENTERED')

"""Front Facing"""

# List of columns to select for Front_Facing_df
selected_columns = [
    'ID_Num',
    'Source',
    'Input Date',
    'First Name',
    'Last Name',
    'Phone Number',
    'Email',
    'Best way to contact',
    'Address (Website)',
    'User Neighborhood',
    'Neighborhood (ArcGIS)',
    'Zip Code',
    'UFPZ (Urban Forest Priority Zone)',
    'HisDis (Historical Disinvestment)',
    'EJ Flag',
    'EJ #',
    'Available Area (sqm)',
    'Landowner?',
    'TES (Census block)',
    'Canopy Percentage (ArcGIS)',
    'Type of Property'
]

# Create Back End Dataframe
Front_Facing_df = new_Full_df[selected_columns].copy()

# Rename columns
new_column_names = {
    'Address (Website)': 'Address',
    'Canopy Percentage (ArcGIS)': 'Canopy Coverage (Parcel)'
}

Front_Facing_df.rename(columns=new_column_names, inplace=True)

# Prep for writing to Google Sheet
sheet = client.open('Yardtree Planting Progress Database Neighborwoods')
Front_Facing = sheet.worksheet('Requested Trees')

# Get data from Requested Trees Sheet
existing_headers = Front_Facing.row_values(1)
existing_data = Front_Facing.get_all_values()
existing_df = pd.DataFrame(existing_data[1:], columns = existing_headers)

# Get columns right
for col in existing_headers:
    if col not in Front_Facing_df.columns:
        Front_Facing_df[col] = None

Front_Facing_df = Front_Facing_df[existing_headers]

# Replace NaN and infinity values with None
Front_Facing_df = Front_Facing_df.replace({np.nan: None, np.inf: None, -np.inf: None})

# Add Not Contacted for Status
Front_Facing_df['Status'] = '1: Not Contacted'

# Convert DataFrame to a list of lists, excluding the header
data_list = Front_Facing_df.values.tolist()

# Find the first empty row in the "Input Date" column
input_date_col_idx = existing_headers_slice.index("Input Date")
start_row = next((idx for idx, row in enumerate(existing_data[1:], start=2) if not row[input_date_col_idx]), len(existing_data) + 1)

# Prepare the range for updating (starting at column A to column Z)
start_col = 'A'
end_col = multiple_letters_list[len(data_list[0])- 1]
range_to_update = f'{start_col}{start_row}:{end_col}{start_row + len(data_list) - 1}'

# Append the data to the Google Sheet without the header
Front_Facing.update(range_name=range_to_update, values=data_list, value_input_option='USER_ENTERED')