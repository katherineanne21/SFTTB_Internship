# -*- coding: utf-8 -*-
"""InitialDataCleanUp_KatherineAnne.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_42fEe068FXuUyQ5kEPf2lwx64RC3OXw

### Prep Workspace
"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

# Import required modules
import pandas as pd
import geopandas as gpd
from shapely.geometry import Point, Polygon, shape
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import numpy as np
import gspread
from oauth2client.service_account import ServiceAccountCredentials
from tabulate import tabulate
import time

# Define the scope
scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]

# Provide the path to the JSON file
creds = ServiceAccountCredentials.from_json_keyfile_name('/content/drive/Shareddrives/SFTT Shared Drive/0General Management & Admin/Employee Onboarding/SFTT Specific Employees/Summer 2024/Katherine Anne/Colabs/enhanced-keel-424914-m2-7c2e82d072b9.json', scope)

# Authorize the clientsheet
client = gspread.authorize(creds)

"""### Set Up Aesthetics"""

# Brand Color Pallete
brand_color_palette = ['#5e9b4f', '#8fc640', '#d8e264']
brand_cmap = mcolors.ListedColormap(brand_color_palette)

# Brand Color Ramp
color_start = '#d8e264'
color_end = '#5e9b4f'
brand_ramp_cmap = mcolors.LinearSegmentedColormap.from_list('brand_color_ramp', [color_start, color_end])

"""### Read In Data"""

# NeighborWoods Data
NeighborWoods_url = 'https://docs.google.com/spreadsheets/d/1enP3gL5AnYsXtrby4jwONOUZMayF4iVG0EGhQRZBrVA/edit#gid=0'
NeighborWoods_Sheets = client.open_by_url(NeighborWoods_url)
NeighborWoods_Sheet2 = NeighborWoods_Sheets.get_worksheet(1)
NeighborWoods_Data2 = NeighborWoods_Sheet2.get_all_values()
NeighborWoods2_df = pd.DataFrame(NeighborWoods_Data2[1:], columns = NeighborWoods_Data2[0])

# TES Data
shapefile_path = '/content/drive/Shareddrives/SFTT Shared Drive/0General Management & Admin/Employee Onboarding/SFTT Specific Employees/Summer 2024/Katherine Anne/Colabs/GIS/TESData/ma_tes.shp'
TESData = gpd.read_file(shapefile_path)

# Arc GIS Data
shapefile_path = '/content/drive/Shareddrives/SFTT Shared Drive/0General Management & Admin/Employee Onboarding/SFTT Specific Employees/Summer 2024/Katherine Anne/Colabs/GIS/ArcGISData/Parcels_Web_V4.shp'
ArcGIS_Data = gpd.read_file(shapefile_path)
ArcGIS_Data['ID_Num'] = range(1, len(ArcGIS_Data) + 1)

# Environmental Justice Populations Data
shapefile_path = '/content/drive/Shareddrives/SFTT Shared Drive/0General Management & Admin/Employee Onboarding/SFTT Specific Employees/Summer 2024/Katherine Anne/Colabs/GIS/ArcGISData/EJPops_Web_Clip.shp'
EJPop_Data = gpd.read_file(shapefile_path)

# Historical Disinvestment/ Redlining Data
shapefile_path =  '/content/drive/Shareddrives/SFTT Shared Drive/0General Management & Admin/Employee Onboarding/SFTT Specific Employees/Summer 2024/Katherine Anne/Colabs/GIS/ArcGISData/cartodb-query.shp'
HistDis_Data = gpd.read_file(shapefile_path)

# New Tree Planting Progress Database
TPP_url = 'https://docs.google.com/spreadsheets/d/15TbiooJKm0yN6qaZsuhguYETzz_M3g4UM5RdwW_Zn3E/edit?gid=0#gid=0'
TPP_Sheets = client.open_by_url(TPP_url)
TPP_Sheet1 = TPP_Sheets.get_worksheet(0)
TPP_Data = TPP_Sheet1.get_all_values()
TPP_df = pd.DataFrame(TPP_Data[1:], columns = TPP_Data[0])

"""###Linking Data

Prepares Dataframe for linking
"""

# Capitalize address column
NeighborWoods2_df['Street Name'] = NeighborWoods2_df['Street Name'].str.upper()

# Remove Don't Read Columns
NeighborWoods2_df = NeighborWoods2_df[NeighborWoods2_df["Don't Read"] != 'TRUE'].copy()

# Define number of rows to shrink by
shrink = 10

# Add a shortened address column to each dataframe
# Eliminates confusion with suffix
NeighborWoods2_df.loc[:, 'short_addr_NW'] = NeighborWoods2_df['Street Name'].str[:shrink]
ArcGIS_Data.loc[:, 'short_str_name_AG'] = ArcGIS_Data['addr_str'].str[:shrink]

# Initialize a column for ID_Num
NeighborWoods2_df['ID_Num'] = np.nan

# Reset index to take care of skipped rows
NeighborWoods2_df.reset_index(drop=True, inplace=True)
idx = 0

"""Creates a function that provides linking column between our data and the ArcGIS data"""

# Create a function that links the tables
def link_address(NeighborWoods2_row, ArcGIS_Data, idx, shrink):
  # Select street number and name
  street_number = NeighborWoods2_row['Street Number']
  str_name_NW = NeighborWoods2_row['Street Name']

  # Shorten street name
  short_str_name_NW = str_name_NW[:shrink]
  ArcGIS_Data.loc[:, 'short_str_name_AG'] = ArcGIS_Data['addr_str'].str[:shrink]

  # Find the rows that contain that street name
  matching_rows = ArcGIS_Data[ArcGIS_Data['short_str_name_AG'].str.contains(short_str_name_NW, na=False)]

  # Debugging output
  # print(f"shrink: {shrink}, short_str_name_NW: {short_str_name_NW}, matching_rows: {len(matching_rows)}")

  # Find the right number
  matching_numbers = matching_rows['addr_num'].str.contains(street_number, na=False)
  matching_numbers_rows = matching_rows[matching_numbers]

  # Narrow down number rows
  # Add ID_Num to NW
  right_index = None

  if len(matching_numbers_rows) > 1:
    if matching_numbers_rows['addr_num'].str.contains(' ', na=False).any():
      individual_numbers = matching_numbers_rows[['addr_num', 'ID_Num']].apply(lambda row: row['addr_num'].split() if row['addr_num'] is not None and ' ' in row['addr_num'] else [row['addr_num']], axis=1)

      for idx2, num_list in individual_numbers.items():
        if street_number in num_list:
          right_index = idx2
          right_ID_Num = matching_numbers_rows.loc[right_index, 'ID_Num']
          return right_ID_Num

    else:
      for idx3, row in matching_numbers_rows.iterrows():
        if street_number == row['addr_num']:
          right_index = idx3
          right_ID_Num = matching_numbers_rows.loc[right_index, 'ID_Num']
          return right_ID_Num

  else:
      if not matching_numbers_rows.empty:
          right_index = matching_numbers_rows.index[0]
          right_ID_Num = matching_numbers_rows.loc[right_index, 'ID_Num']
          return right_ID_Num

  # Add NaN value if no address was found
  if right_index is None:
      return np.NaN

# Create a vector that starts at 10 and goes to 5
shrink_range = range(10, 5, -1)

# Iterate through the database multiple times to make sure large names and small names find matches
# Only update rows that don't have a number
for shrink in shrink_range:
  for idx, row in NeighborWoods2_df.iterrows():
    if pd.isna(NeighborWoods2_df.at[idx, 'ID_Num']):
      NeighborWoods2_row = NeighborWoods2_df.loc[idx]
      id_num = link_address(NeighborWoods2_row, ArcGIS_Data, idx, shrink)
      NeighborWoods2_df.at[idx, 'ID_Num'] = id_num

#print(NeighborWoods2_df[['Street Name', 'ID_Num']])

"""Geo Merges"""

# Merge TES data

# Ensure both GeoDataFrames use the same CRS
TESData = TESData.to_crs(ArcGIS_Data.crs)

# Perform a spatial join
merged_gdf = gpd.sjoin(ArcGIS_Data, TESData, how = 'left', predicate = 'intersects')

# Merge EJ Pop Data

# Ensure both GeoDataFrames use the same CRS
EJPop_Data = EJPop_Data.to_crs(merged_gdf.crs)

# Perform a spatial join
merged_gdf = gpd.sjoin(merged_gdf, EJPop_Data, how='left', predicate='intersects', lsuffix='_left', rsuffix='_right')

# Merge Historical Disinvestment Data

HistDis_Data = HistDis_Data.to_crs(merged_gdf.crs)

# Perform a spatial join
merged_gdf = gpd.sjoin(merged_gdf, HistDis_Data, how='left', predicate='intersects', lsuffix='_l', rsuffix='_r')

"""Full merge of all columns"""

# Ensure the ID_Num column in both DataFrames is of the same type
NeighborWoods2_df['ID_Num'] = NeighborWoods2_df['ID_Num'].astype('Int64')
merged_gdf['ID_Num'] = merged_gdf['ID_Num'].astype('Int64')

# Merge the dataframes on the ID_Num column
Full_df = NeighborWoods2_df.merge(merged_gdf, on='ID_Num', how='left', suffixes=('_NW', '_AG'))

# print(Full_df[['site_addr', 'ID_Num']])

"""###Create columns"""

# Add in website for all rows under the Source column
Full_df['Source (website, Tree Alliance, event sign up)'] = 'Website'

# Get rid of selected in Neighborhood column

# Change City column to first letter capitals
Full_df['CITY'] = Full_df['CITY'].str.title()

# Create a list of acceptable neighborhoods
neighborhood_list = ['Dorchester','Hyde Park', 'Roxbury', 'Mattapan']

for idx, row in Full_df.iterrows():
  if 'Selected' in row['Neighborhood']:
    Full_df.at[idx, 'Neighborhood'] = Full_df.at[idx, 'CITY']
  if row['Neighborhood'] not in neighborhood_list:
    Full_df.at[idx, 'Neighborhood'] = 'Other'

# Change Landowner to True False
for idx, row in Full_df.iterrows():
  if 'Yes' in row['Are you the landowner?']:
    Full_df.at[idx, 'Are you the landowner?'] = 'TRUE'
  else:
    Full_df.at[idx, 'Are you the landowner?'] = 'FALSE'

# Add Available Area (sqm) column
Full_df['Lot Area (sqm)'] = (Full_df['sqm_imperv'] * 100)/ Full_df['pct_imperv']
Full_df['Available Area (sqm)'] = Full_df['Lot Area (sqm)'] - Full_df['sqm_imperv']

# Change PrioZone to True False

# Convert column to string
Full_df['PrioZone'] = Full_df['PrioZone'].astype(str)

for idx, row in Full_df.iterrows():
    prio_zone_value = row['PrioZone']
    if 'Y' in prio_zone_value:
      Full_df.at[idx, 'PrioZone'] = 'TRUE'
    elif 'N' in prio_zone_value:
      Full_df.at[idx, 'PrioZone'] = 'FALSE'
    else:
      Full_df.at[idx, 'PrioZone'] = None

# Create a column for EJ Flag and EJ #
Full_df['EJ Flag'] = 'FALSE'
Full_df['EJ #'] = None

Full_df['EJ'] = Full_df['EJ'].astype(str)

for idx, row in Full_df.iterrows():
  if 'Yes' in row['EJ']:
    Full_df.at[idx, 'EJ Flag'] = 'TRUE'
  if pd.notnull(row['EJ_CRITE_1']):
    Full_df.at[idx, 'EJ #'] = row['EJ_CRITE_1']

# Create historical disinvestment flag and letter columns
Full_df['HisDis (Historical Disinvestment)'] = 'FALSE'
Full_df['HisDis Letter'] = None

Full_df['holc_grade__r'] = Full_df['holc_grade__r'].astype(str)

for idx, row in Full_df.iterrows():
  if pd.notnull(row['holc_grade__r']):
    Full_df.at[idx, 'HisDis Letter'] = row['holc_grade__r']
  if row['holc_grade__r'] in ['C', 'D']:
    Full_df.at[idx, 'HisDis (Historical Disinvestment)'] = 'TRUE'

# Handle Duplicate Rows

# Filter out rows with duplicate dates
Full_df = Full_df.drop_duplicates(subset='Date of Submission via website or maunal sign up', keep='first')

# Reset index
Full_df = Full_df.reset_index(drop=True)

"""###Rename columns"""

# Choose columns to save
selected_columns = [
    'Source (website, Tree Alliance, event sign up)',
    'Date of Submission via website or maunal sign up',
    'First Name',
    'Last Name',
    'Address of planting location *\n',
    'Neighborhood',
    'Zip',
    'Email',
    'Phone',
    'Best way to contact',
    'Number of trees',
    'Are you the landowner?',
    'MAP_PAR_ID',
    'muni',
    'site_addr',
    'addr_str',
    'addr_num',
    'addr_zip',
    'CanopyPerc',
    'CITY',
    'Lot Area (sqm)',
    'pct_imperv',
    'tes',
    'PrioZone',
    'HisDis (Historical Disinvestment)',
    'EJ Flag',
    'EJ #',
    'Available Area (sqm)',
    'HisDis Letter',
    'LU_Recode_',
    'ID_Num',
    'sqm_imperv']

new_Full_df = Full_df[selected_columns].copy()

# Rename columns
new_column_names = {
    'Source (website, Tree Alliance, event sign up)': 'Source',
    'Date of Submission via website or maunal sign up': 'Input Date',
    'First Name': 'First Name',
    'Last Name': 'Last Name',
    'Address of planting location *\n': 'Address (NeighborWoods)',
    'Neighborhood': 'Neighborhood',
    'Zip': 'Zip Code',
    'Email': 'Email',
    'Phone': 'Phone Number',
    'Best way to contact': 'Best way to contact',
    'Number of trees': 'Number of trees',
    'Are you the landowner?': 'Landowner?',
    'ID_Num': 'ID_Num',
    'MAP_PAR_ID': 'Map Parcel ID',
    'muni': 'Municipality',
    'site_addr': 'Site Address (ArcGIS)',
    'addr_str': 'Street Name (ArcGIS)',
    'addr_num': 'Street Number (ArcGIS)',
    'addr_zip': 'Street Zip (ArcGIS)',
    'sqm_imperv': 'Impervious Area (sqm)',
    'CanopyPerc': 'Canopy Percentage (ArcGIS)',
    'CITY': 'City (ArcGIS)',
    'Lot Area (sqm)': 'Lot Area (sqm)',
    'pct_imperv': 'Impervious Percentage (ArcGIS)',
    'tes': 'TES (Census block)',
    'PrioZone': 'UFPZ (Urban Forest Priority Zone)',
    'HisDis (Historical Disinvestment)': 'HisDis (Historical Disinvestment)',
    'EJ Flag': 'EJ Flag',
    'EJ #': 'EJ #',
    'Available Area (sqm)': 'Available Area (sqm)',
    'HisDis Letter': 'HisDis Letter',
    'LU_Recode_': 'Type of Property'
}

new_Full_df.rename(columns=new_column_names, inplace=True)

"""### Writing Data to Sheet

Back End
"""

# List of columns to select for Back_End_df
selected_columns = [
    'ID_Num',
    'Source',
    'Input Date',
    'First Name',
    'Last Name',
    'Phone Number',
    'Email',
    'Best way to contact',
    'Address (NeighborWoods)',
    'Site Address (ArcGIS)',
    'Street Number (ArcGIS)',
    'Street Name (ArcGIS)',
    'Neighborhood',
    'City (ArcGIS)',
    'Municipality',
    'Zip Code',
    'Street Zip (ArcGIS)',
    'Map Parcel ID',
    'UFPZ (Urban Forest Priority Zone)',
    'HisDis (Historical Disinvestment)',
    'HisDis Letter',
    'EJ Flag',
    'EJ #',
    'Available Area (sqm)',
    'Lot Area (sqm)',
    'Impervious Area (sqm)',
    'Impervious Percentage (ArcGIS)',
    'Landowner?',
    'TES (Census block)',
    'Canopy Percentage (ArcGIS)',
    'Type of Property'
]

# Create Back End Dataframe
Back_End_df = new_Full_df[selected_columns].copy()

# Prep for writing to Google Sheet
sheet = client.open('Tree Planting Progress Database')
Back_End = sheet.worksheet('Back End')

# Get columns right
existing_headers = Back_End.row_values(1)

for col in existing_headers:
    if col not in Back_End_df.columns:
        Back_End_df[col] = None

Back_End_df = Back_End_df[existing_headers]

# Replace NaN and infinity values with None
Back_End_df = Back_End_df.replace({np.nan: None, np.inf: None, -np.inf: None})

# Convert DataFrame to a list of lists, excluding the header
data_list = Back_End_df.values.tolist()

# Append the data to the Google Sheet without the header
Back_End.append_rows(data_list, value_input_option='USER_ENTERED')

"""Front Facing"""

# List of columns to select for Front_Facing_df
selected_columns = [
    'ID_Num',
    'Source',
    'Input Date',
    'First Name',
    'Last Name',
    'Phone Number',
    'Email',
    'Best way to contact',
    'Address (NeighborWoods)',
    'Neighborhood',
    'Zip Code',
    'UFPZ (Urban Forest Priority Zone)',
    'HisDis (Historical Disinvestment)',
    'EJ Flag',
    'EJ #',
    'Available Area (sqm)',
    'Landowner?',
    'TES (Census block)',
    'Canopy Percentage (ArcGIS)',
    'Type of Property'
]

# Create Back End Dataframe
Front_Facing_df = new_Full_df[selected_columns].copy()

# Rename columns
new_column_names = {
    'Address (NeighborWoods)': 'Address',
    'Canopy Percentage (ArcGIS)': 'Canopy Coverage (Parcel)'
}

Front_Facing_df.rename(columns=new_column_names, inplace=True)

# Prep for writing to Google Sheet
sheet = client.open('Tree Planting Progress Database')
Front_Facing = sheet.worksheet('Front Facing')

# Load the existing data from the Google Sheet into a DataFrame
existing_data = Front_Facing.get_all_values()
existing_headers = existing_data[0]
existing_df = pd.DataFrame(existing_data[1:], columns=existing_headers)

for col in existing_headers:
    if col not in Front_Facing_df.columns:
        Front_Facing_df[col] = None

Front_Facing_df = Front_Facing_df[existing_headers]

# Replace NaN and infinity values with None
Front_Facing_df = Front_Facing_df.replace({np.nan: None, np.inf: None, -np.inf: None})

# Add Not Contacted for Status
Front_Facing_df['Status'] = '1: Not Contacted'

# Convert DataFrame to a list of lists, excluding the header
data_list = Front_Facing_df.values.tolist()

# Find the first empty row in the "Input Date" column
input_date_col_idx = existing_headers.index("Input Date")
first_empty_date_row_idx = next((idx for idx, row in enumerate(existing_data[1:], start=2) if not row[input_date_col_idx]), len(existing_data) + 1)

# Batch size for appending rows
batch_size = 50

# Append data in batches
for start in range(0, len(data_list), batch_size):
    end = start + batch_size
    batch_data = data_list[start:end]
    Front_Facing.insert_rows(batch_data, row=first_empty_date_row_idx + start, value_input_option='USER_ENTERED')
    time.sleep(2)  # Delay to prevent hitting rate limits

# Append each row from data_list to the appropriate index (no header)
# for i, row in enumerate(data_list):
    # Front_Facing.insert_row(row, index=first_empty_date_row_idx + i, value_input_option='USER_ENTERED')

"""### Testing

I want to create a df called Full_df that has all of the columns from FrontFacing_df and BackEnd_df. I will also list all of the columns already in Full_df.

Full_df:


Front Facing columns:
Source	Status	Input Date	First Name	Last Name	Phone Number	Email	Best way to contact	Address	Neighborhood	Zip Code	Landowner Info	Number of trees	Google Satellite View	Worth a Site Vist?	Plant Nursery	Signed Agreement	Date Planted	Lat	Long	UFPZ (Urban Forest Priority Zone)	HisDis (Historical Disinvestment)	EJ Flag	EJ #	Available Area (sqm)	Landowner?	TES (Census block)	Canopy Coverage (Parcel)	Type of Property	Read.In	ID_Num
"""

# Read a df to Testing to see it's contents

# Select df
# df = new_Full_df.copy()

# Change to string to avoid errors
new_Full_df = new_Full_df.astype(str)

# Replace NaN and infinity values with None
new_Full_df.replace({np.nan: None, np.inf: None, -np.inf: None}, inplace=True)

# Open the Google Sheet
sheet = client.open('Tree Planting Progress Database')
worksheet = sheet.worksheet('Testing')

# Clear the sheet
worksheet.clear()

# Convert the DataFrame to a list of lists
data_list = [new_Full_df.columns.tolist()] + new_Full_df.values.tolist()

# Write the data to the Google Sheet
worksheet.append_rows(data_list, value_input_option='USER_ENTERED')

pd.set_option('display.max_columns', None)
Full_df.head()

filtered_row = ArcGIS_Data[ArcGIS_Data['CanopyPerc'] == 30.671782]
print(filtered_row)

print(ArcGIS_Data.columns.tolist())
print(TESData.columns.tolist())

print(Full_df.columns)
print(Full_df[['Address (NeighborWoods)', 'Source']])

print(ArcGIS_Data.columns.tolist())

for idx, row in NeighborWoods2_df.iterrows():
  # Select row
  street_number = row['Street Number']
  short_str_name_NW = row['short_addr_NW']

  # Find the rows that contain that street name
  matching_rows = ArcGIS_Data[ArcGIS_Data['short_str_name_AG'].str.contains(short_str_name_NW, na=False)]

  # Find the right number
  matching_numbers = matching_rows['addr_num'].str.contains(street_number, na=False)
  matching_numbers_rows = matching_rows[matching_numbers]

  # Narrow down number rows
  # Add ID_Num to NW
  right_index = None

  if len(matching_numbers_rows) > 1:
    if matching_numbers_rows['addr_num'].str.contains(' ', na=False).any():
      individual_numbers = matching_numbers_rows[['addr_num', 'ID_Num']].apply(lambda row: row['addr_num'].split() if row['addr_num'] is not None and ' ' in row['addr_num'] else [row['addr_num']], axis=1)

      for idx2, num_list in individual_numbers.items():
        if street_number in num_list:
          right_index = idx2
          right_ID_Num = matching_numbers_rows.loc[right_index, 'ID_Num']
          NeighborWoods2_df.at[idx, 'ID_Num'] = right_ID_Num
          break

    else:
      for idx3, row in matching_numbers_rows.iterrows():
        if street_number == row['addr_num']:
          right_index = idx3
          right_ID_Num = matching_numbers_rows.loc[right_index, 'ID_Num']
          NeighborWoods2_df.at[idx, 'ID_Num'] = right_ID_Num
          break

  else:
      if not matching_numbers_rows.empty:
          right_index = matching_numbers_rows.index[0]
          right_ID_Num = matching_numbers_rows.loc[right_index, 'ID_Num']
          NeighborWoods2_df.at[idx, 'ID_Num'] = right_ID_Num

  # Add NaN value if no address was found
  if right_index is None:
      NeighborWoods2_df.at[idx, 'ID_Num'] = np.NaN

print(NeighborWoods2_df[['Street Name', 'ID_Num']])

# Capitalize address column
NeighborWoods2_df['Street Name'] = NeighborWoods2_df['Street Name'].str.upper()

# Remove Don't Read Columns
NeighborWoods2_df = NeighborWoods2_df[NeighborWoods2_df["Don't Read"] != 'TRUE']

# Define number of rows to shrink by
shrink = 4

# Add a shortened address column to each dataframe
# Eliminates confusion with suffix
NeighborWoods2_df['short_addr_NW'] = NeighborWoods2_df['Street Name'].str[:shrink]
ArcGIS_Data['short_str_name_AG'] = ArcGIS_Data['addr_str'].str[:shrink]

# Initialize a column for ID_Num
NeighborWoods2_df['ID_Num'] = np.nan

# Test Row Index
row_index = 0

# Select row
street_number = NeighborWoods2_df.iloc[row_index]['Street Number']
short_str_name_NW = NeighborWoods2_df.iloc[row_index]['short_addr_NW']

# Find the rows that contain that street name
matching_rows = ArcGIS_Data[ArcGIS_Data['short_str_name_AG'] == short_str_name_NW]

# Find the right number
matching_numbers = matching_rows['addr_num'].str.contains(street_number, na=False)
matching_numbers_rows = matching_rows[matching_numbers]

# Debugging
print("Matching rows:")
print(matching_numbers_rows[['site_addr', 'ID_Num']])

# Narrow down number rows
# Add ID_Num to NW
right_index = None

if len(matching_numbers_rows) > 1:
  if matching_numbers_rows['addr_num'].str.contains(' ', na=False).any():
    individual_numbers = matching_numbers_rows[['addr_num', 'ID_Num']].apply(lambda row: row['addr_num'].split() if row['addr_num'] is not None and ' ' in row['addr_num'] else [row['addr_num']], axis=1)

    # Debugging
    print("individual_numbers:")
    print(individual_numbers)

    for idx2, num_list in individual_numbers.items():
      if street_number in num_list:
        right_index = idx2
        right_ID_Num = matching_numbers_rows.loc[right_index, 'ID_Num']
        NeighborWoods2_df.at[row_index, 'ID_Num'] = right_ID_Num
        break

  else:
    for idx3, row in matching_numbers_rows.iterrows():
      if street_number == row['addr_num']:
        right_index = idx3
        right_ID_Num = matching_numbers_rows.loc[right_index, 'ID_Num']
        NeighborWoods2_df.at[row_index, 'ID_Num'] = right_ID_Num
        break

else:
    if not matching_numbers_rows.empty:
        right_index = matching_numbers_rows.index[0]
        right_ID_Num = matching_numbers_rows.loc[right_index, 'ID_Num']
        NeighborWoods2_df.at[row_index, 'ID_Num'] = right_ID_Num

# Debugging
print("Index:")
print(right_index)
print("ID_Num")
print(right_ID_Num)

# Add NaN value if no address was found
if right_index is None:
    NeighborWoods2_df.at[row_index, 'ID_Num'] = np.NaN

print(NeighborWoods2_df[['Street Name', 'ID_Num']])

column_name = 'site_addr'
value_to_find = '65 WELLES AV'

# Find rows where the column matches the value
matching_rows = ArcGIS_Data[ArcGIS_Data[column_name] == value_to_find]
selected_columns = matching_rows[['site_addr', 'pct_imperv']]
print(selected_columns)

# Extract the last two letters of each address
ArcGIS_Data['last_two_letters'] = ArcGIS_Data['site_addr'].str[-2:]

# Find unique last two letters
unique_last_two_letters = ArcGIS_Data['last_two_letters'].unique()

# Count the number of unique last two letters
num_unique_last_two_letters = len(unique_last_two_letters)

# Print the number of unique last two letters and the unique values
print(f"Number of unique last two letters: {num_unique_last_two_letters}")
print("Unique last two letters:")
print(unique_last_two_letters)

# intersection = gpd.overlay(MassGISData, TESData, how='intersection')

# fig, ax = plt.subplots(figsize=(10, 10))
# MassGISData.plot(column='POLY_TYPE', ax=ax, cmap='brand_ramp_cmap', legend=True)
# plt.show()

# pd.set_option('display.max_columns', None)  # Show all columns
pd.set_option('display.max_rows', None)     # Show all rows

print(tabulate(ArcGIS_Data, headers='keys', tablefmt='psql'))

# Reset display options to default
pd.reset_option('display.max_columns')
pd.reset_option('display.max_rows')