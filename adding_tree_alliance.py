# -*- coding: utf-8 -*-
"""Adding_Tree_Alliance.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pmTd7WFp88nckN6ckgC_3PnmxP6FPxP8

### Prep Workspace
"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

# Import required modules
import pandas as pd
import numpy as np
import gspread
import string
from oauth2client.service_account import ServiceAccountCredentials

# Define the scope
scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]

# Provide the path to the JSON file
creds = ServiceAccountCredentials.from_json_keyfile_name('/content/drive/Shareddrives/SFTT Shared Drive/NeighborWoods Data Management/enhanced-keel-424914-m2-7c2e82d072b9.json', scope)

# Authorize the clientsheet
client = gspread.authorize(creds)

"""### Read In Data"""

# Read in the merged geodatabases

file_path = '/content/drive/Shareddrives/SFTT Shared Drive/NeighborWoods Data Management/Merged_Geodatabse.csv'

dtype_spec = {
    'addr_str': str,
    'addr_num': str,
    'MAP_PAR_ID': str,
    'muni': str,
    'site_addr': str,
    'addr_zip': str,
    'tes': str,
    'LU_Recode_': str,
}

merged_df = pd.read_csv(file_path, dtype=dtype_spec, low_memory=False)

# New Tree Planting Progress Database
TPP_url = 'https://docs.google.com/spreadsheets/d/15TbiooJKm0yN6qaZsuhguYETzz_M3g4UM5RdwW_Zn3E/edit?gid=0#gid=0'
TPP_Sheets = client.open_by_url(TPP_url)

TPP_Sheet4 = TPP_Sheets.worksheet("Tree Alliance Trees")

TPP_Website = TPP_Sheet4.get_all_values()

TPP_Website_df = pd.DataFrame(TPP_Website[1:], columns = TPP_Website[0])

"""###Linking Data

Prepares Dataframe for linking
"""

# Capitalize address column
TPP_Website_df['User Street Name'] = TPP_Website_df['User Street Name'].str.upper()

# Remove columns that have already been read in
TPP_Website_df = TPP_Website_df[TPP_Website_df['Input Date'] != ''].copy() # filters out empty rows from having the checkbox
TPP_Website_df_new = TPP_Website_df[TPP_Website_df["Read.In"] != 'TRUE'].copy()

# Define number of letters to shrink by
shrink = 10

# Add a shortened address column to each dataframe
# Eliminates confusion with suffix
TPP_Website_df_new.loc[:, 'short_addr_website'] = TPP_Website_df_new['User Street Name'].str[:shrink]
merged_df.loc[:, 'short_str_name_AG'] = merged_df['addr_str'].str[:shrink]

# Initialize a column for ID_Num
TPP_Website_df_new['ID_Num'] = np.nan

# Reset index to take care of skipped rows
TPP_Website_df_new.reset_index(drop=True, inplace=True)
idx = 0

"""Flags all columns as having been read in"""

# Set all columns as read in
TPP_Website_df["Read.In"] = 'TRUE'

# Open Google Sheet
sheet = client.open('Yardtree Planting Progress Database Neighborwoods')
worksheet = sheet.worksheet('Tree Alliance Trees')

# Create a list of lists
data_list = [TPP_Website_df.columns.tolist()] + TPP_Website_df.values.tolist()

# Clear the sheet
worksheet.update(values=[[''] * worksheet.row_count] * worksheet.row_count)

# Write the data to Google Sheet
worksheet.append_rows(data_list, value_input_option='USER_ENTERED')

"""Creates a function that provides linking column between our data and the merged geodatabase data"""

# Create a function that links the tables
def link_address(TPP_Website_row, ArcGIS_Data, idx, shrink):
  # Select street number and name
  street_number = TPP_Website_row['User Street Number']
  str_name_NW = TPP_Website_row['User Street Name']

  # Shorten street name
  short_str_name_NW = str_name_NW[:shrink]
  merged_df.loc[:, 'short_str_name_AG'] = ArcGIS_Data['addr_str'].str[:shrink]

  # Find the rows that contain that street name
  matching_rows = merged_df[merged_df['short_str_name_AG'].str.contains(short_str_name_NW, na=False)]

  # Find the right number
  matching_numbers = matching_rows['addr_num'].str.contains(street_number, na=False)
  matching_numbers_rows = matching_rows[matching_numbers]

  # Narrow down number rows
  # Add ID_Num to NW
  right_index = None

  if len(matching_numbers_rows) > 1:
    if matching_numbers_rows['addr_num'].str.contains(' ', na=False).any():
      individual_numbers = matching_numbers_rows[['addr_num', 'ID_Num']].apply(lambda row: row['addr_num'].split() if row['addr_num'] is not None and ' ' in row['addr_num'] else [row['addr_num']], axis=1)

      for idx2, num_list in individual_numbers.items():
        if street_number in num_list:
          right_index = idx2
          right_ID_Num = matching_numbers_rows.loc[right_index, 'ID_Num']
          return right_ID_Num

    else:
      for idx3, row in matching_numbers_rows.iterrows():
        if street_number == row['addr_num']:
          right_index = idx3
          right_ID_Num = matching_numbers_rows.loc[right_index, 'ID_Num']
          return right_ID_Num

  else:
      if not matching_numbers_rows.empty:
          right_index = matching_numbers_rows.index[0]
          right_ID_Num = matching_numbers_rows.loc[right_index, 'ID_Num']
          return right_ID_Num

  # Add NaN value if no address was found
  if right_index is None:
      return np.NaN

# Create a vector that starts at 10 and goes to 5
shrink_range = range(10, 5, -1)

# Iterate through the database multiple times to make sure large names and small names find matches
# Only update rows that don't have a number
for shrink in shrink_range:
  for idx, row in TPP_Website_df_new.iterrows():
    if pd.isna(TPP_Website_df_new.at[idx, 'ID_Num']):
      TPP_Website_row = TPP_Website_df_new.loc[idx]
      id_num = link_address(TPP_Website_row, merged_df, idx, shrink)
      TPP_Website_df_new.at[idx, 'ID_Num'] = id_num

"""Full merge of all columns"""

# Ensure the ID_Num column in both DataFrames is of the same type
TPP_Website_df_new['ID_Num'] = TPP_Website_df_new['ID_Num'].astype('Int64')
merged_df['ID_Num'] = merged_df['ID_Num'].astype('Int64')

# Merge the dataframes on the ID_Num column
Full_df = TPP_Website_df_new.merge(merged_df, on='ID_Num', how='left', suffixes=('_NW', '_AG'))

"""###Create columns"""

# Add in website for all rows under the Source column
Full_df['Source'] = 'Tree Alliance'

# Change City column to first letter capitals
Full_df['CITY'] = Full_df['CITY'].str.title()

# Add Available Area (sqm) column
Full_df['Lot Area (sqm)'] = (Full_df['sqm_imperv'] * 100)/ Full_df['pct_imperv']
Full_df['Available Area (sqm)'] = Full_df['Lot Area (sqm)'] - Full_df['sqm_imperv']

# Combine street number and name
Full_df['User Address'] = Full_df['User Street Number'].astype(str) + ' ' + Full_df['User Street Name']

# Handle Duplicate Rows

# Filter out rows with duplicate dates
Full_df = Full_df.drop_duplicates(subset='Input Date', keep='first')

# Reset index
Full_df = Full_df.reset_index(drop=True)

"""###Rename columns"""

# Choose columns to save
selected_columns = [
    'Source',
    'Input Date',
    'First Name',
    'Last Name',
    'User Address',
    'User Neighborhood',
    'User Zip',
    'Email',
    'Phone',
    'MAP_PAR_ID',
    'muni',
    'site_addr',
    'addr_str',
    'addr_num',
    'addr_zip',
    'CanopyPerc',
    'CITY',
    'Lot Area (sqm)',
    'pct_imperv',
    'tes',
    'PrioZone',
    'HisDis (Historical Disinvestment)',
    'EJ Flag',
    'EJ #',
    'Available Area (sqm)',
    'HisDis Letter',
    'LU_Recode_',
    'ID_Num',
    'sqm_imperv']

new_Full_df = Full_df[selected_columns].copy()

# Rename columns
new_column_names = {
    'Source': 'Source',
    'Input Date': 'Input Date',
    'First Name': 'First Name',
    'Last Name': 'Last Name',
    'User Address': 'Address (Website)',
    'User Neighborhood': 'User Neighborhood',
    'User Zip': 'Zip Code',
    'Email': 'Email',
    'Phone': 'Phone Number',
    'Number of trees': 'Number of trees',
    'ID_Num': 'ID_Num',
    'MAP_PAR_ID': 'Map Parcel ID',
    'muni': 'Municipality',
    'site_addr': 'Site Address (ArcGIS)',
    'addr_str': 'Street Name (ArcGIS)',
    'addr_num': 'Street Number (ArcGIS)',
    'addr_zip': 'Street Zip (ArcGIS)',
    'sqm_imperv': 'Impervious Area (sqm)',
    'CanopyPerc': 'Canopy Percentage (ArcGIS)',
    'CITY': 'Neighborhood (ArcGIS)',
    'Lot Area (sqm)': 'Lot Area (sqm)',
    'pct_imperv': 'Impervious Percentage (ArcGIS)',
    'tes': 'TES (Census block)',
    'PrioZone': 'UFPZ (Urban Forest Priority Zone)',
    'HisDis (Historical Disinvestment)': 'HisDis (Historical Disinvestment)',
    'EJ Flag': 'EJ Flag',
    'EJ #': 'EJ #',
    'Available Area (sqm)': 'Available Area (sqm)',
    'HisDis Letter': 'HisDis Letter',
    'LU_Recode_': 'Type of Property'
}

new_Full_df.rename(columns=new_column_names, inplace=True)

"""### Writing Data to Sheet"""

# Create a list of letters that rolls over to AA

multiple_letters_list = []
for i in range(1, 3):  # Go through once for singletons and a second time for doubles
        for letter in string.ascii_uppercase: # Repeat for each uppercase letter
            if i == 1:
                multiple_letters_list.append(letter)
            else:
                for letter2 in string.ascii_uppercase: # Repeat for each uppercase letter
                    multiple_letters_list.append(letter + letter2)

"""Back End"""

# List of columns to select for Back_End_df
selected_columns = [
    'ID_Num',
    'Source',
    'Input Date',
    'First Name',
    'Last Name',
    'Phone Number',
    'Email',
    'Address (Website)',
    'Site Address (ArcGIS)',
    'Street Number (ArcGIS)',
    'Street Name (ArcGIS)',
    'User Neighborhood',
    'Neighborhood (ArcGIS)',
    'Municipality',
    'Zip Code',
    'Street Zip (ArcGIS)',
    'Map Parcel ID',
    'UFPZ (Urban Forest Priority Zone)',
    'HisDis (Historical Disinvestment)',
    'HisDis Letter',
    'EJ Flag',
    'EJ #',
    'Available Area (sqm)',
    'Lot Area (sqm)',
    'Impervious Area (sqm)',
    'Impervious Percentage (ArcGIS)',
    'TES (Census block)',
    'Canopy Percentage (ArcGIS)',
    'Type of Property'
]

# Create Back End Dataframe
Back_End_df = new_Full_df[selected_columns].copy()

# Prep for writing to Google Sheet
sheet = client.open('Yardtree Planting Progress Database Neighborwoods')
Back_End = sheet.worksheet('Back End - NeighborWoods')

# Get data from Back End Sheet
existing_headers = Back_End.row_values(1)
existing_data = Back_End.get_all_values()

# Slice data to select the data from just the B:AE rows
start_col_index = 1  # Column B
end_col_index = 31  # Column AE

# Slice headers to include columns B through AE
existing_headers_slice = existing_headers[start_col_index:end_col_index + 1]

# Slice data to include columns B through AE
existing_data_slice = [row[start_col_index:end_col_index + 1] for row in existing_data]

existing_df = pd.DataFrame(existing_data_slice[1:], columns = existing_headers_slice)

# Get columns right
for col in existing_headers_slice:
    if col not in Back_End_df.columns:
        Back_End_df[col] = None

Back_End_df = Back_End_df[existing_headers_slice]

# Replace NaN and infinity values with None
Back_End_df = Back_End_df.replace({np.nan: None, np.inf: None, -np.inf: None})

# Convert DataFrame to a list of lists, excluding the header
data_list = Back_End_df.values.tolist()

# Find the first empty row in the "Input Date" column
input_date_col_idx = existing_headers_slice.index("Input Date")
start_row = next((idx for idx, row in enumerate(existing_data[1:], start=2) if not row[input_date_col_idx]), len(existing_data) + 1)

# Prepare the range for updating (starting at column B)
start_col = 'B'
end_col = multiple_letters_list[len(data_list[0])]
range_to_update = f'{start_col}{start_row}:{end_col}{start_row + len(data_list) - 1}'

# Append the data to the Google Sheet without the header
Back_End.update(range_to_update, data_list, value_input_option='USER_ENTERED')

"""Front Facing"""

# List of columns to select for Front_Facing_df
selected_columns = [
    'ID_Num',
    'Source',
    'Input Date',
    'First Name',
    'Last Name',
    'Phone Number',
    'Email',
    'Address (Website)',
    'User Neighborhood',
    'Neighborhood (ArcGIS)',
    'Zip Code',
    'UFPZ (Urban Forest Priority Zone)',
    'HisDis (Historical Disinvestment)',
    'EJ Flag',
    'EJ #',
    'Available Area (sqm)',
    'TES (Census block)',
    'Canopy Percentage (ArcGIS)',
    'Type of Property'
]

# Create Back End Dataframe
Front_Facing_df = new_Full_df[selected_columns].copy()

# Rename columns
new_column_names = {
    'Address (Website)': 'Address',
    'Canopy Percentage (ArcGIS)': 'Canopy Coverage (Parcel)'
}

Front_Facing_df.rename(columns=new_column_names, inplace=True)

# Prep for writing to Google Sheet
sheet = client.open('Yardtree Planting Progress Database Neighborwoods')
Front_Facing = sheet.worksheet('Requested Trees')

# Get data from Requested Trees Sheet
existing_headers = Front_Facing.row_values(1)
existing_data = Front_Facing.get_all_values()

# Slice data to select the data from just the A:Z rows
start_col_index = 0  # Column A
end_col_index = 25  # Column Z

# Slice headers to include columns A through Z
existing_headers_slice = existing_headers[start_col_index:end_col_index + 1]

# Slice data to include columns A through Z
existing_data_slice = [row[start_col_index:end_col_index + 1] for row in existing_data]

existing_df = pd.DataFrame(existing_data_slice[1:], columns = existing_headers_slice)

print(existing_df.columns)

# Get columns right
for col in existing_headers_slice:
    if col not in Front_Facing_df.columns:
        Front_Facing_df[col] = None

Front_Facing_df = Front_Facing_df[existing_headers_slice]

# Replace NaN and infinity values with None
Front_Facing_df = Front_Facing_df.replace({np.nan: None, np.inf: None, -np.inf: None})

# Add Not Contacted for Status
Front_Facing_df['Status'] = '1: Not Contacted'

# Convert DataFrame to a list of lists, excluding the header
data_list = Front_Facing_df.values.tolist()

# Find the first empty row in the "Input Date" column
input_date_col_idx = existing_headers_slice.index("Input Date")
start_row = next((idx for idx, row in enumerate(existing_data[1:], start=2) if not row[input_date_col_idx]), len(existing_data) + 1)

# Prepare the range for updating (starting at column A to column Z)
start_col = 'A'
end_col = multiple_letters_list[len(data_list[0])- 1]
range_to_update = f'{start_col}{start_row}:{end_col}{start_row + len(data_list) - 1}'

# Append the data to the Google Sheet without the header
Front_Facing.update(range_name=range_to_update, values=data_list, value_input_option='USER_ENTERED')